{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is the role of filters and feature maps in Convolutional Neural Network (CNN)?\n",
        "- Filters (or Kernels):\n",
        "  - Role:\n",
        "\n",
        "  - I) They are used to detect specific patterns in the input, such as edges, textures, or shapes.\n",
        "\n",
        "  - II) Each filter is responsible for detecting a particular feature (e.g., vertical edge, horizontal edge, a specific texture).\n",
        "\n",
        "- Feature Maps:\n",
        "  - Role:\n",
        "\n",
        "  - I) Represent where specific features are detected in the input.\n",
        "\n",
        "  - II) Each feature map corresponds to one filter and highlights the presence of the corresponding feature in various spatial locations.\n",
        "\n",
        "2. Explain the concepts of padding and stride in CNNs(Convolutional Neural Network). How do they affect the output dimensions of feature maps?\n",
        "- Padding:\n",
        "  - Padding is the process of adding extra pixels (usually zeros) around the border of the input image before applying the convolution operation.\n",
        "- Why do we use Padding\n",
        "\n",
        "  - To control the size of the output feature map.\n",
        "\n",
        "  - To preserve spatial dimensions (so the output does not shrink too quickly).\n",
        "\n",
        "  - To allow filters to process edge pixels properly.\n",
        "- How Padding Affects Output Size:\n",
        "\n",
        "  - More Padding (larger P) → Larger Output Size\n",
        "  - Example:\n",
        "  - Same padding ensures that the output size stays the same as the input.\n",
        "\n",
        "- Stride:\n",
        "  - Stride is the number of pixels by which the filter moves (or “slides”) over the input image.\n",
        "- How Stride Affects Output Size:\n",
        "\n",
        "  - Smaller Stride (S = 1) → Larger Output (more detailed)\n",
        "\n",
        "  - Larger Stride (S > 1) → Smaller Output (downsampling effect)\n",
        "\n",
        "3. Define receptive field in the context of CNNs. Why is it important for deep architectures?\n",
        "- The receptive field of a neuron in a Convolutional Neural Network (CNN) refers to the region (patch of pixels) in the input image that affects the value of that neuron.\n",
        "\n",
        "  - In simple terms:\n",
        "\n",
        "  - For early layers, the receptive field is small — each neuron “sees” only a small part of the input image (e.g., a 3×3 region).\n",
        "\n",
        "  - As we go deeper into the network, the receptive field grows larger because neurons aggregate information from previous layers.\n",
        "\n",
        "- Why Is the Receptive Field Important\n",
        "\n",
        "- I) Capturing Context:\n",
        "\n",
        "  - A larger receptive field allows the network to capture more global and contextual information.\n",
        "\n",
        "  - For tasks like object detection or image classification, understanding the full object requires looking at a wider region of the image.\n",
        "\n",
        "- II) Hierarchical Feature Learning:\n",
        "\n",
        "  - Early layers detect simple features (edges, textures) from small regions.\n",
        "\n",
        "  - Deeper layers detect complex features (object parts, shapes) by combining information from a larger receptive field.\n",
        "\n",
        "- III) Deep Architectures Benefit:\n",
        "\n",
        "  - By stacking multiple convolutional layers, the receptive field grows exponentially.\n",
        "\n",
        "  - This enables neurons in deep layers to “see” large portions of the original image, essential for tasks that depend on global structure.\n",
        "\n",
        "4. Discuss how filter size and stride influence the number of parameters in a CNN.\n",
        "- Filter Size:\n",
        "- Directly influences parameter count:\n",
        "  - The number of trainable parameters in a single convolutional filter is calculated by multiplying its height, width, and the depth of the input data, plus one bias term.\n",
        "  - Example:\n",
        "A 3x3 filter for a 3-channel input would have (3 * 3 * 3) + 1 = 28 parameters.\n",
        "\n",
        "- Stride:\n",
        "- No influence on parameter count:\n",
        "  - The stride, or the number of pixels the filter moves across the input, is a separate hyperparameter that does not add or remove any learnable weights from the filter itself.\n",
        "- Impact on output size:\n",
        "  - A larger stride reduces the spatial dimensions (height and width) of the output feature map, as the filter moves further with each step.\n",
        "\n",
        "5. Compare and contrast different CNN-based architectures like LeNet,\n",
        "AlexNet, and VGG in terms of depth, filter sizes, and performance.\n",
        "- 1. LeNet (LeNet-5):\n",
        "\n",
        "- Depth:\n",
        "  - Relatively shallow (7 layers: 2 convolutional + 3 fully connected layers)\n",
        "- Filter Size:\n",
        "\t- Small filters (e.g., 5×5)\n",
        "- Stride & Padding:\n",
        "\t- Small strides (usually 1), no advanced padding techniques\n",
        "- Performance:\n",
        "\t- Suitable for small datasets (MNIST). Very fast to train but not suitable for large-scale image tasks (e.g., ImageNet).\n",
        "\n",
        "- 2. AlexNet:\n",
        "- Depth:\n",
        "\t- Deeper than LeNet (8 layers: 5 convolutional + 3 fully connected layers)\n",
        "- Filter:\n",
        "  - Size\tLarger initial filter size: First layer uses 11×11 filters with stride 4\n",
        "- Use of ReLU:\n",
        "\t- First architecture to popularize ReLU activation for non-linearity.\n",
        "- Performance:\n",
        "\t- Achieved breakthrough performance in ImageNet (won in 2012). Handles large-scale datasets well.\n",
        "\n",
        "- 3. VGG (VGG-16 / VGG-19):\n",
        "- Depth:\n",
        "\t- Much deeper (VGG-16: 16 layers; VGG-19: 19 layers)\n",
        "- Filter:\n",
        "  - Size\tVery small filters: 3×3 filters consistently across all layers.\n",
        "- Uniform Architecture:\n",
        "\t- Uses simple and repetitive structure: stacked small convolution layers + pooling.\n",
        "- Performance:\n",
        "  - Very strong accuracy on ImageNet. Known for good generalization and simplicity of design.\n",
        "\n"
      ],
      "metadata": {
        "id": "1kFPQ40-dfEf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fprtLv9jdYc9",
        "outputId": "6243d2c7-9695-4e8b-b6d6-8b40c74bafd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 92ms/step - accuracy: 0.8392 - loss: 0.5231 - val_accuracy: 0.9812 - val_loss: 0.0650\n",
            "Epoch 2/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 90ms/step - accuracy: 0.9795 - loss: 0.0641 - val_accuracy: 0.9800 - val_loss: 0.0671\n",
            "Epoch 3/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 91ms/step - accuracy: 0.9860 - loss: 0.0463 - val_accuracy: 0.9863 - val_loss: 0.0459\n",
            "Epoch 4/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 89ms/step - accuracy: 0.9892 - loss: 0.0339 - val_accuracy: 0.9902 - val_loss: 0.0381\n",
            "Epoch 5/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 90ms/step - accuracy: 0.9925 - loss: 0.0243 - val_accuracy: 0.9897 - val_loss: 0.0375\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9864 - loss: 0.0385\n",
            "\n",
            "Test Loss: 0.0291, Test Accuracy: 0.9903\n"
          ]
        }
      ],
      "source": [
        "# 6. Using keras, build and train a simple CNN model on the MNIST dataset from scratch. Include code for module creation, compilation, training, and evaluation.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1️⃣ Load MNIST Dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize input images to [0, 1] and reshape to (28,28,1)\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "x_test  = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test  = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# 2️⃣ Build CNN Model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# 3️⃣ Compile the Model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 4️⃣ Train the Model\n",
        "model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=5,\n",
        "    batch_size=128,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 5️⃣ Evaluate the Model\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
        "print(f'\\nTest Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Load and preprocess the CIFAR-10 dataset using Keras, and create a CNN model to classify RGB images. Show your preprocessing and architecture.\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# 1️⃣ Load and Preprocess CIFAR-10 Dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "# 2️⃣ Build CNN Model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# 3️⃣ Compile Model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 4️⃣ Train Model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# 5️⃣ Evaluate Model\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XZ9sbID3i35",
        "outputId": "eda55eda-1805-4255-83f1-74647ecf4ca9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 0us/step\n",
            "Epoch 1/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 74ms/step - accuracy: 0.3484 - loss: 1.7882 - val_accuracy: 0.5518 - val_loss: 1.2997\n",
            "Epoch 2/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 69ms/step - accuracy: 0.5476 - loss: 1.2700 - val_accuracy: 0.6140 - val_loss: 1.1191\n",
            "Epoch 3/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 70ms/step - accuracy: 0.6107 - loss: 1.1007 - val_accuracy: 0.6232 - val_loss: 1.0858\n",
            "Epoch 4/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 69ms/step - accuracy: 0.6483 - loss: 1.0154 - val_accuracy: 0.6206 - val_loss: 1.0939\n",
            "Epoch 5/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 69ms/step - accuracy: 0.6766 - loss: 0.9304 - val_accuracy: 0.6692 - val_loss: 0.9541\n",
            "Epoch 6/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 70ms/step - accuracy: 0.6968 - loss: 0.8731 - val_accuracy: 0.6712 - val_loss: 0.9475\n",
            "Epoch 7/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 70ms/step - accuracy: 0.7086 - loss: 0.8343 - val_accuracy: 0.6900 - val_loss: 0.9206\n",
            "Epoch 8/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 69ms/step - accuracy: 0.7273 - loss: 0.7746 - val_accuracy: 0.6678 - val_loss: 0.9664\n",
            "Epoch 9/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 68ms/step - accuracy: 0.7396 - loss: 0.7505 - val_accuracy: 0.6678 - val_loss: 0.9883\n",
            "Epoch 10/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 69ms/step - accuracy: 0.7566 - loss: 0.7041 - val_accuracy: 0.6820 - val_loss: 0.9566\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.6774 - loss: 0.9707\n",
            "Test accuracy: 0.6739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Using PyTorch, write a script to define and train a CNN on the MNIST dataset. Include model definition, data loaders, training loop, and accuracy evaluation.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ✅ 1️⃣ Device Configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ✅ 2️⃣ Data Loading and Preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Mean & Std for MNIST\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# ✅ 3️⃣ Define CNN Model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool  = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1   = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2   = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)  # Flatten\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN().to(device)\n",
        "\n",
        "# ✅ 4️⃣ Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# ✅ 5️⃣ Training Loop\n",
        "for epoch in range(5):  # Train for 5 epochs\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/5], Loss: {running_loss/len(train_loader):.4f}')\n",
        "\n",
        "# ✅ 6️⃣ Evaluate Accuracy\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'\\nTest Accuracy: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODXwZk1W4ZHB",
        "outputId": "74596e14-b30f-4fa0-845c-a51f26791b1d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.45MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 160kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.51MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.27MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.1391\n",
            "Epoch [2/5], Loss: 0.0421\n",
            "Epoch [3/5], Loss: 0.0286\n",
            "Epoch [4/5], Loss: 0.0203\n",
            "Epoch [5/5], Loss: 0.0160\n",
            "\n",
            "Test Accuracy: 98.85%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Given a custom image dataset stored in a local directory, write code using Keras ImageDataGenerator to preprocess and train a CNN model.\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# 1️⃣ Preprocessing: Rescale images to [0,1]\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen   = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# 2️⃣ Load Data from Directory\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/kaggle/input/chest-xray-pneumonia',        # Path to training images\n",
        "    target_size=(64, 64),   # Resize images\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'  # For multi-class classification\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    '/content/data/MNIST/raw/train-images-idx3-ubyte.gz',   # Path to validation images\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# 3️⃣ Simple CNN Model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# 4️⃣ Compile Model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 5️⃣ Train Model\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator\n",
        ")"
      ],
      "metadata": {
        "id": "TdTOAsvuBg5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Data Generators\n",
        "train_gen = ImageDataGenerator(rescale=1./255, horizontal_flip=True)\n",
        "val_gen   = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_data = train_gen.flow_from_directory(\n",
        "    'dataset/train',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "val_data = val_gen.flow_from_directory(\n",
        "    'dataset/val',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# CNN Model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train Model\n",
        "model.fit(train_data, epochs=10, validation_data=val_data)\n",
        "\n",
        "# Save Trained Model\n",
        "model.save('pneumonia_classifier.h5')"
      ],
      "metadata": {
        "id": "tNHZEU0t8AMo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}